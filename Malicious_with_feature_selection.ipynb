{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "t_BUdHub254G"
   },
   "outputs": [],
   "source": [
    "#list of useful imports that  I will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score,log_loss,roc_auc_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OqsrYJOVItA",
    "outputId": "83fb7f3e-3a01-4ea8-f2cc-bf1776e7205a"
   },
   "outputs": [],
   "source": [
    "#pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5KtLYvVeVMG-"
   },
   "outputs": [],
   "source": [
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oiFfUnT-4sBN"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\91960\\\\Music\\\\PROJECT\\\\ITML12_Malicious_detection(NA)\\\\DATASET\\\\data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25068\\3203870260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data using pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0murldata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\91960\\Music\\PROJECT\\ITML12_Malicious_detection(NA)\\DATASET\\data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\91960\\\\Music\\\\PROJECT\\\\ITML12_Malicious_detection(NA)\\\\DATASET\\\\data.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data using pandas\n",
    "urldata = pd.read_csv(r'C:\\Users\\91960\\Music\\PROJECT\\ITML12_Malicious_detection(NA)\\DATASET\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "3u9tEhzZ5HcG",
    "outputId": "428379aa-b518-4424-e964-9b8db94d1342"
   },
   "outputs": [],
   "source": [
    "urldata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ3k-obA5QLQ",
    "outputId": "68d645b1-8efb-4375-d2b4-3d0ea6e22240"
   },
   "outputs": [],
   "source": [
    "urldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tABPTUYpKnow",
    "outputId": "56ecd4a2-d979-4f37-d82d-00292bd27858"
   },
   "outputs": [],
   "source": [
    "urldata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1ckRGzVKtn2",
    "outputId": "8c0a681d-e78d-40d8-9f3c-53123b719f7d"
   },
   "outputs": [],
   "source": [
    "#Checking Missing Values\n",
    "urldata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5y4tTnd576Y",
    "outputId": "938d0ad2-cea1-4b33-a97e-b82b254c9b7c"
   },
   "outputs": [],
   "source": [
    "urldata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fYxNatpT5V1M",
    "outputId": "c65c7f16-9c64-4e99-fbce-77a8ad04709d"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Pie(labels=['good', 'bad'], values=[urldata['label'].value_counts()[0], urldata['label'].value_counts()[1]])])\n",
    "fig.update_layout(title='Percentage of data points for ecah class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9r_fbobqYohu",
    "outputId": "6627f18a-ca9d-4f3c-ada9-f1beeb2cdcb4"
   },
   "outputs": [],
   "source": [
    "# !pip install tld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T75lAK5E8PZJ"
   },
   "source": [
    "## 1. DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXaL1HV98UQ8"
   },
   "source": [
    "The following features will be extracted from the URL for classification.\n",
    "1. Length Features\n",
    "\n",
    "  i. Length Of Url\n",
    "\n",
    "  ii. Length of Hostname\n",
    "\n",
    "  iii. Length Of Path\n",
    "\n",
    "  iv. Length Of First Directory\n",
    "\n",
    "  v. Length Of Top Level Domain\n",
    "\n",
    "2. Count Features\n",
    "\n",
    "  i. Count Of '-'\n",
    "\n",
    "  ii. Count Of '@'\n",
    "\n",
    "  iii. Count Of '?'\n",
    "\n",
    "  iv. Count Of '%'\n",
    "\n",
    "  v. Count Of '.'\n",
    "\n",
    "  vi. Count Of '='\n",
    "\n",
    "  vii.Count Of 'http'\n",
    "\n",
    "  viii. Count Of 'www'\n",
    "\n",
    "  ix. Count Of Digits\n",
    "\n",
    "  x. Count Of Letters\n",
    "\n",
    "  xi. Count Of Number Of Directories\n",
    "\n",
    "3. Binary Features\n",
    "\n",
    "  i. Use of IP or not\n",
    "\n",
    "  ii. Use of Shortening URL or not\n",
    "\n",
    "Apart from the lexical features, we will use TFID - Term Frequency Inverse Document as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pxReHZTK_P8"
   },
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "from urllib.parse import urlparse\n",
    "from tld import get_tld\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uROcgvti9v7J"
   },
   "source": [
    "### Length Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpZMoB1FLD33"
   },
   "outputs": [],
   "source": [
    "#Length of URL\n",
    "urldata['url_length'] = urldata['url'].apply(lambda i: len(str(i)))\n",
    "#Hostname Length\n",
    "urldata['hostname_length'] = urldata['url'].apply(lambda i: len(urlparse(i).netloc))\n",
    "#Path Length\n",
    "urldata['path_length'] = urldata['url'].apply(lambda i: len(urlparse(i).path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "fHP5oES3LLME",
    "outputId": "0d3d613b-3230-4ef6-a847-96af298bae6a"
   },
   "outputs": [],
   "source": [
    "urldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG-MP8f9LPpl"
   },
   "outputs": [],
   "source": [
    "#First Directory Length\n",
    "def fd_length(url):\n",
    "    urlpath= urlparse(url).path\n",
    "    try:\n",
    "        return len(urlpath.split('/')[1])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "urldata['fd_length'] = urldata['url'].apply(lambda i: fd_length(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "rSSf65iKLV3f",
    "outputId": "91e707ae-25c5-4cf2-9dfc-13007db86f9c"
   },
   "outputs": [],
   "source": [
    "urldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6wWXKOtLY-M"
   },
   "outputs": [],
   "source": [
    "#Length of Top Level Domain\n",
    "urldata['tld'] = urldata['url'].apply(lambda i: get_tld(i,fail_silently=True))\n",
    "def tld_length(tld):\n",
    "    try:\n",
    "        return len(tld)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "urldata['tld_length'] = urldata['tld'].apply(lambda i: tld_length(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "QZoBuvGuLf-y",
    "outputId": "f8303ef9-d59b-48ae-81fd-7aca03017997"
   },
   "outputs": [],
   "source": [
    "urldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdpuJpgJLgnF",
    "outputId": "7f64c24e-5cc9-4621-910d-4b3894401b2b"
   },
   "outputs": [],
   "source": [
    "urldata['tld'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9SwutCRL97Q",
    "outputId": "0d226753-1d4d-4dda-851f-c711bc091203"
   },
   "outputs": [],
   "source": [
    "len(urldata['tld']== None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq9FW6Q2LwKy",
    "outputId": "4b4547da-52b0-43e6-81b6-6239f2906cda"
   },
   "outputs": [],
   "source": [
    "urldata['tld_length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5PUkK6rMJ3g"
   },
   "outputs": [],
   "source": [
    "urldata = urldata.drop(\"tld\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "vVG4OPr9MKy2",
    "outputId": "02af5a44-c5d6-41df-ed02-4d718bb8861e"
   },
   "outputs": [],
   "source": [
    "#Dataset after extracting length features\n",
    "urldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "411k_wid97yJ"
   },
   "source": [
    "###  Count Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHTvQUGpMSOn"
   },
   "outputs": [],
   "source": [
    "urldata['count-'] = urldata['url'].apply(lambda i: i.count('-'))\n",
    "urldata['count@'] = urldata['url'].apply(lambda i: i.count('@'))\n",
    "urldata['count?'] = urldata['url'].apply(lambda i: i.count('?'))\n",
    "urldata['count%'] = urldata['url'].apply(lambda i: i.count('%'))\n",
    "urldata['count.'] = urldata['url'].apply(lambda i: i.count('.'))\n",
    "urldata['count='] = urldata['url'].apply(lambda i: i.count('='))\n",
    "urldata['count-http'] = urldata['url'].apply(lambda i : i.count('http'))\n",
    "urldata['count-https'] = urldata['url'].apply(lambda i : i.count('https'))\n",
    "urldata['count-www'] = urldata['url'].apply(lambda i: i.count('www'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUA8OsjZMw-3"
   },
   "outputs": [],
   "source": [
    "def digit_count(url):\n",
    "    digits = 0\n",
    "    for i in url:\n",
    "        if i.isnumeric():\n",
    "            digits = digits + 1\n",
    "    return digits\n",
    "urldata['count-digits']= urldata['url'].apply(lambda i: digit_count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_4msL6OMxng"
   },
   "outputs": [],
   "source": [
    "def letter_count(url):\n",
    "    letters = 0\n",
    "    for i in url:\n",
    "        if i.isalpha():\n",
    "            letters = letters + 1\n",
    "    return letters\n",
    "urldata['count-letters']= urldata['url'].apply(lambda i: letter_count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5jKS20YM117"
   },
   "outputs": [],
   "source": [
    "def no_of_dir(url):\n",
    "    urldir = urlparse(url).path\n",
    "    return urldir.count('/')\n",
    "urldata['count_dir'] = urldata['url'].apply(lambda i: no_of_dir(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "9ucmTOSTM4xL",
    "outputId": "e29daf64-f3a7-4782-9220-86c40de4f0bf"
   },
   "outputs": [],
   "source": [
    "# Data after extracting Count Features\n",
    "urldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2Sg2WK5-I3f"
   },
   "source": [
    "### Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9ocC5h1M730"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iRFhM_HNBo3"
   },
   "outputs": [],
   "source": [
    "#Use of IP or not in domain\n",
    "def having_ip_address(url):\n",
    "    match = re.search(\n",
    "        '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "        '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
    "        '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)' # IPv4 in hexadecimal\n",
    "        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6\n",
    "    if match:\n",
    "        # print match.group()\n",
    "        return -1\n",
    "    else:\n",
    "        # print 'No matching pattern found'\n",
    "        return 1\n",
    "urldata['use_of_ip'] = urldata['url'].apply(lambda i: having_ip_address(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMe_xEa0NFYx"
   },
   "outputs": [],
   "source": [
    "def shortening_service(url):\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                      'tr\\.im|link\\.zip\\.net',\n",
    "                      url)\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "urldata['short_url'] = urldata['url'].apply(lambda i: shortening_service(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "Me3-sFN5NJUm",
    "outputId": "bac50e93-662a-4cbe-d1f7-bcd8ab9190cf"
   },
   "outputs": [],
   "source": [
    "#Data after extracting Binary Features\n",
    "urldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YAKWD-3tzNo",
    "outputId": "0500f374-b4a0-4d34-ceb5-b08e7737b280"
   },
   "outputs": [],
   "source": [
    "urldata['label'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uitoqVcjuaYZ",
    "outputId": "dd08a5a4-7aa9-4426-9d1d-2c5fe12a1bbc"
   },
   "outputs": [],
   "source": [
    "# Upsampling\n",
    "from sklearn.utils import resample\n",
    "df_majority = urldata[urldata.label == 'good']\n",
    "df_minarity = urldata[urldata.label == 'bad']\n",
    "\n",
    "df_upsample = resample(df_minarity, n_samples = 100000, replace = True)\n",
    "df_downsample = resample(df_majority, n_samples = 100000, replace = False)\n",
    "\n",
    "urldata = pd.concat([df_upsample,df_downsample],axis =0)\n",
    "\n",
    "urldata.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAHJ8Q5QpLpI"
   },
   "outputs": [],
   "source": [
    "# shuffle the DataFrame rows \n",
    "urldata = urldata.sample(frac = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "tKdDhwKforY7",
    "outputId": "76e1d2f2-dc17-4b6a-a1ae-6077df675134"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Pie(labels=['good', 'bad'], values=[urldata['label'].value_counts()[0], urldata['label'].value_counts()[1]])])\n",
    "fig.update_layout(title='Percentage of data points for ecah class after upsampling')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-Zqr5LqVi_O"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "urldata['label'] = le.fit_transform(urldata['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uniIy_qeNLml",
    "outputId": "8d4e56f0-f8d5-4787-c649-7f45755b518b"
   },
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "corrmat = urldata.corr()\n",
    "f, ax = plt.subplots(figsize=(25,19))\n",
    "sns.heatmap(corrmat, square=True, annot = True, annot_kws={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "IFj44-3zTEk7",
    "outputId": "1a3e53dc-e732-4931-95df-0ed59a15fa46"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.countplot(x='label',data=urldata)\n",
    "plt.title(\"Count Of URLs\",fontsize=20)\n",
    "plt.xlabel(\"Type Of URLs\",fontsize=18)\n",
    "plt.ylabel(\"Number Of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "8C8Hh8L7TOgH",
    "outputId": "589706ce-98a2-417e-e3bf-822a9787aaf3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.hist(urldata['url_length'],bins=50,color='LightBlue')\n",
    "plt.title(\"URL-Length\",fontsize=20)\n",
    "plt.xlabel(\"Url-Length\",fontsize=18)\n",
    "plt.ylabel(\"Number Of Urls\",fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ndmP3Nq4TizB",
    "outputId": "a3865efb-a5ee-4a54-b262-8d40257f35f8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.hist(urldata['hostname_length'],bins=50,color='Lightgreen')\n",
    "plt.title(\"Hostname-Length\",fontsize=20)\n",
    "plt.xlabel(\"Length Of Hostname\",fontsize=18)\n",
    "plt.ylabel(\"Number Of Urls\",fontsize=18)\n",
    "plt.ylim(0,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "7RgSCziMTosB",
    "outputId": "29de26f4-4594-44a9-d221-a409654b61ba"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.hist(urldata['tld_length'],bins=50,color='Lightgreen')\n",
    "plt.title(\"TLD-Length\",fontsize=20)\n",
    "plt.xlabel(\"Length Of TLD\",fontsize=18)\n",
    "plt.ylabel(\"Number Of Urls\",fontsize=18)\n",
    "plt.ylim(0,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "CEOxBCyLTw3-",
    "outputId": "1687199c-06c3-4f2b-c005-8477b6c4a23d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Number Of Directories In Url\",fontsize=20)\n",
    "sns.countplot(x='count_dir',data=urldata)\n",
    "plt.xlabel(\"Number Of Directories\",fontsize=18)\n",
    "plt.ylabel(\"Number Of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "o7skrVzoUI4R",
    "outputId": "9f1d70f9-2624-431f-80d0-de51ef11ce3a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Number Of Directories In Url\",fontsize=20)\n",
    "sns.countplot(x='count_dir',data=urldata,hue='label')\n",
    "plt.xlabel(\"Number Of Directories\",fontsize=18)\n",
    "plt.ylabel(\"Number Of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "6MXkmt9EUQGL",
    "outputId": "aa6476b5-4d8b-4e9f-e770-dba37d6a268b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of IP In Url\",fontsize=20)\n",
    "plt.xlabel(\"Use Of IP\",fontsize=18)\n",
    "\n",
    "sns.countplot(urldata['use_of_ip'])\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "XC_wz-iyUZIN",
    "outputId": "4fcf6373-32b3-40c6-bc83-a119c87b1bf1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of IP In Url\",fontsize=20)\n",
    "plt.xlabel(\"Use Of IP\",fontsize=18)\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)\n",
    "sns.countplot(urldata['use_of_ip'],hue='label',data=urldata)\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "m0LJ2euBUeZo",
    "outputId": "e39684cb-09c1-4fcb-c2fd-1083208188bf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of http In Url\",fontsize=20)\n",
    "plt.xlabel(\"Use Of IP\",fontsize=18)\n",
    "plt.ylim((0,1000))\n",
    "sns.countplot(urldata['count-http'])\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "z9UU1oyGUiFm",
    "outputId": "e4c4efc0-016f-453d-85ed-24402d9dff08"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of http In Url\",fontsize=20)\n",
    "plt.xlabel(\"Count Of http\",fontsize=18)\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)\n",
    "plt.ylim((0,1000))\n",
    "sns.countplot(urldata['count-http'],hue='label',data=urldata)\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "t6FSMn2wUoFx",
    "outputId": "086d5791-8476-47e1-f285-ed5fbbd6839e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of http In Url\",fontsize=20)\n",
    "plt.xlabel(\"Count Of http\",fontsize=18)\n",
    "\n",
    "sns.countplot(urldata['count-http'],hue='label',data=urldata)\n",
    "\n",
    "plt.ylabel(\"Number of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "HftIz7BgUuwA",
    "outputId": "7c304513-03b1-4354-dc80-43a5c1407a25"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of WWW In URL\",fontsize=20)\n",
    "plt.xlabel(\"Count Of WWW\",fontsize=18)\n",
    "sns.countplot(urldata['count-www'])\n",
    "plt.ylim(0,1000)\n",
    "plt.ylabel(\"Number Of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "lo5N5bhcUztI",
    "outputId": "72ca05db-3085-45bb-cc98-ab923ff3bb0c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Use Of WWW In URL\",fontsize=20)\n",
    "plt.xlabel(\"Count Of WWW\",fontsize=18)\n",
    "\n",
    "sns.countplot(urldata['count-www'],hue='label',data=urldata)\n",
    "plt.ylim(0,1000)\n",
    "plt.ylabel(\"Number Of URLs\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOVjt0IWU2h3"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMOVKe-SU-BX"
   },
   "outputs": [],
   "source": [
    "#Predictor Variables\n",
    "x = urldata[['hostname_length',\n",
    "       'path_length', 'fd_length', 'tld_length', 'count-', 'count@', 'count?',\n",
    "       'count%', 'count.', 'count=', 'count-http','count-https', 'count-www', 'count-digits',\n",
    "       'count-letters', 'count_dir', 'use_of_ip']]\n",
    "\n",
    "#Target Variable\n",
    "y = urldata['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0T-YbIXiYfag",
    "outputId": "18019365-393f-48d5-da49-031f539b143a"
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWRna2wuWJEA",
    "outputId": "ada15b0d-0b70-46c1-dbfc-ec30b1546f28"
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI7J20E8WM6f"
   },
   "outputs": [],
   "source": [
    "#Splitting the data into Training and Testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y,stratify = y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOT0HjSqYa1W",
    "outputId": "142484a4-2512-4853-956e-9fba2e3f881b"
   },
   "outputs": [],
   "source": [
    "print(\"X_Train data shape\",X_train.shape)\n",
    "print(\"y_train data shape\",y_train.shape)\n",
    "print(\"X_Test data shape\",X_test.shape)\n",
    "print(\"y_test data shape\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymPLpZWrYbVh"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQREkJGXaQYM"
   },
   "source": [
    "### Removing features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI-cCn8jYbVy"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.4)\n",
    "selector.fit(X_train)\n",
    "X_train_var = selector.transform(X_train)\n",
    "X_test_var = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfodONrgYbV8",
    "outputId": "fe8881d2-6a85-4951-af30-93ee07bf0e88"
   },
   "outputs": [],
   "source": [
    "print(X_train_var.shape)\n",
    "print(X_test_var.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzaRz4wjaLNj"
   },
   "source": [
    "### Univariate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KuOZbyjbpOR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train)\n",
    "X_train = minmax.transform(X_train)\n",
    "X_test = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYMsKIp9ZyQy"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# k = Number of top features to select\n",
    "uni = SelectKBest(chi2, k=10)\n",
    "uni.fit(X_train, y_train)\n",
    "X_train_uni= uni.transform(X_train)\n",
    "X_test_uni = uni.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlyq3i-acSAA",
    "outputId": "7a437e12-9eda-4924-f8d3-64fb89c6439a"
   },
   "outputs": [],
   "source": [
    "print(X_train_uni.shape)\n",
    "print(X_test_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs3bPI21p8-k"
   },
   "outputs": [],
   "source": [
    "def plotErrors(k,train,cv):\n",
    "       \n",
    "    plt.plot(k, train, label='Train logloss')\n",
    "    plt.plot(k, cv, label='CV logloss')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"log(C)= -log(λ)\")\n",
    "    plt.ylabel(\"Neg_Log Loss\")\n",
    "    plt.title(\"Error Plot for Train and Validation data\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siPlVAtcrkOC"
   },
   "source": [
    "## Random Forest with hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nERUuTkFrjgF",
    "outputId": "3bc40016-e23b-4029-edf9-1695e5468abb"
   },
   "outputs": [],
   "source": [
    "#Random Forest with Removing features with low variance features\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_var, y_train)\n",
    "\n",
    "rfc_predictions = rfc.predict(X_test_var)\n",
    "accuracy_score(y_test, rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wnV-I6lrjgV",
    "outputId": "2c5fbaab-7c1d-45ad-d5d9-f7735d4d225e"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc1RULXycrEv",
    "outputId": "1ac8b0b2-bbe7-4337-9af0-7c1bd4c9aae6"
   },
   "outputs": [],
   "source": [
    "#Random Forest univariate feature selection\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_uni, y_train)\n",
    "\n",
    "rfc_predictions = rfc.predict(X_test_uni)\n",
    "accuracy_score(y_test, rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpOuh1nJc1Sv",
    "outputId": "c054f037-0209-4bde-f846-a99cd026840f"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_W2WJdPrZjl"
   },
   "source": [
    "## Logistic Regression with Removing features with low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "8B9246yDqCZH",
    "outputId": "53f441f7-8311-4cfc-9053-500fee605b2e"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#For hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "parameters={'C':[10**-6,10**-5,10**-4, 10**-2, 10**0, 10**2, 10**3] }\n",
    "log_c = list(map(lambda x : float(math.log(x)),parameters['C']))\n",
    "\n",
    "clf_log = LogisticRegression(penalty='l2',class_weight='balanced')\n",
    "\n",
    "clf = GridSearchCV(clf_log, parameters, cv=5, scoring='neg_log_loss',return_train_score =True)\n",
    "clf.fit(X_train_var, y_train)\n",
    "\n",
    "train_loss= clf.cv_results_['mean_train_score']\n",
    "cv_loss = clf.cv_results_['mean_test_score'] \n",
    "\n",
    "plotErrors(k=log_c,train=train_loss,cv=cv_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qyu_R_CTqW4N",
    "outputId": "243e3f70-b150-4019-a8c2-cbcb585c77a2"
   },
   "outputs": [],
   "source": [
    "clf_log = clf.best_estimator_\n",
    "clf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwIMMCTTqa2C"
   },
   "outputs": [],
   "source": [
    "# #Trainig with the best value of C\n",
    "# clf_log.fit(X_train_var, y_train)\n",
    "# filename = r'C:\\Users\\ST-0008\\Desktop\\ITML12_Malicious_detection(NA)\\FINAL CODE\\FRONT END\\Malicious\\malicious_log_var.pkl'\n",
    "# pickle.dump(clf_log, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGMExyabqfU1",
    "outputId": "54e07410-1dbe-401a-f3a7-5dbeb06726be"
   },
   "outputs": [],
   "source": [
    "#Printing the log-loss for both trian and test data\n",
    "train_loss = log_loss(y_train, clf_log.predict_proba(X_train_var)[:,1])\n",
    "test_loss  =log_loss(y_test, clf_log.predict_proba(X_test_var)[:,1])\n",
    "\n",
    "\n",
    "print(\"Log_loss on train data is :{}\".format(train_loss))\n",
    "print(\"Log_loss on test data is :{}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "UtZu6jMeqmUC",
    "outputId": "1327868d-9049-471f-dd89-65d5d8662b99"
   },
   "outputs": [],
   "source": [
    "#Testing AUC on Test data\n",
    "pred_test = clf_log.predict_proba(X_test_var)[:,1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_test)\n",
    "pred_train = clf_log.predict_proba(X_train_var)[:,1]\n",
    "fpr2,tpr2,thresholds2 = roc_curve(y_train,pred_train)\n",
    "\n",
    "#plot ROC curve\n",
    "x = plt.subplot( )\n",
    "x.plot(fpr1, tpr1, label ='Test ROC ,auc='+str(roc_auc_score(y_test,pred_test)))\n",
    "x.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred_train)))\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "x.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC on Test data is \" +str(roc_auc_score(y_test,pred_test)))\n",
    "print(\"AUC on Train data is \" +str(roc_auc_score(y_train,pred_train)))\n",
    "\n",
    "print\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = le.classes_\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(y_test, pred_test.round()), index=class_names, columns=class_names )\n",
    "fig = plt.figure( )\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "6N8d8zMIv9po",
    "outputId": "d0ae1458-075f-49de-bbff-ea08bc406cbe"
   },
   "outputs": [],
   "source": [
    "original =  [\"Good\" if x==1 else \"Bad\" for x in y_test[:20]]\n",
    "predicted = clf_log.predict(X_test_var[:20])\n",
    "pred = []\n",
    "\n",
    "for i in predicted:\n",
    "  if i == 1:\n",
    "    k = \"Good\"\n",
    "    pred.append(k)\n",
    "  else:\n",
    "    k = \"Bad\"\n",
    "    pred.append(k)\n",
    "\n",
    "# dictionary of lists  \n",
    "dict = {'original_classlabel': original, 'predicted_classlabel': pred} \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttMDMNoIv5MZ"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['model', 'Classifier' , 'Train-Auc', 'Test-Auc','Tain-loss','Test-loss'])\n",
    "new = ['Logistic Regression with Removing features with low variance features','LogisticRegression',0.7438,0.7438,0.5892,0.5892]\n",
    "results.loc[0] = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rJ-91x8etkZ"
   },
   "source": [
    "## Logistic Regression with Univariate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e89X9occetka",
    "outputId": "b7590b4c-a896-44c5-b34c-061669623cd6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#For hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "parameters={'C':[10**-6,10**-5,10**-4, 10**-2, 10**0, 10**2, 10**3] }\n",
    "log_c = list(map(lambda x : float(math.log(x)),parameters['C']))\n",
    "\n",
    "clf_log = LogisticRegression(penalty='l2',class_weight='balanced')\n",
    "\n",
    "clf = GridSearchCV(clf_log, parameters, cv=5, scoring='neg_log_loss',return_train_score =True)\n",
    "clf.fit(X_train_uni, y_train)\n",
    "\n",
    "train_loss= clf.cv_results_['mean_train_score']\n",
    "cv_loss = clf.cv_results_['mean_test_score'] \n",
    "\n",
    "plotErrors(k=log_c,train=train_loss,cv=cv_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJwmQS8Retkf",
    "outputId": "5170e07c-30d8-4219-c827-ad2fa6aedcfc"
   },
   "outputs": [],
   "source": [
    "clf_log = clf.best_estimator_\n",
    "clf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-o54KSXetkk"
   },
   "outputs": [],
   "source": [
    "# #Trainig with the best value of C\n",
    "# clf_log.fit(X_train_uni, y_train)\n",
    "# filename = r'C:\\Users\\ST-0008\\Desktop\\ITML12_Malicious_detection(NA)\\FINAL CODE\\FRONT END\\Malicious\\malicious_log_uni.pkl'\n",
    "# pickle.dump(clf_log, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zp6gRZSietko",
    "outputId": "1cd2dbd6-a379-4b87-d793-b6eb3bbfee41"
   },
   "outputs": [],
   "source": [
    "#Printing the log-loss for both trian and test data\n",
    "train_loss = log_loss(y_train, clf_log.predict_proba(X_train_uni)[:,1])\n",
    "test_loss  =log_loss(y_test, clf_log.predict_proba(X_test_uni)[:,1])\n",
    "\n",
    "\n",
    "print(\"Log_loss on train data is :{}\".format(train_loss))\n",
    "print(\"Log_loss on test data is :{}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "70zHkzFzetkt",
    "outputId": "ce618c71-4faf-41fc-e261-392f40b9bd43"
   },
   "outputs": [],
   "source": [
    "#Testing AUC on Test data\n",
    "pred_test = clf_log.predict_proba(X_test_uni)[:,1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_test)\n",
    "pred_train = clf_log.predict_proba(X_train_uni)[:,1]\n",
    "fpr2,tpr2,thresholds2 = roc_curve(y_train,pred_train)\n",
    "\n",
    "#plot ROC curve\n",
    "x = plt.subplot( )\n",
    "x.plot(fpr1, tpr1, label ='Test ROC ,auc='+str(roc_auc_score(y_test,pred_test)))\n",
    "x.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred_train)))\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "x.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC on Test data is \" +str(roc_auc_score(y_test,pred_test)))\n",
    "print(\"AUC on Train data is \" +str(roc_auc_score(y_train,pred_train)))\n",
    "\n",
    "print\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = le.classes_\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(y_test, pred_test.round()), index=class_names, columns=class_names )\n",
    "fig = plt.figure( )\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "TF1Y6dQyetkx",
    "outputId": "f9bca278-3379-4bbf-a20b-7a77fc91c4bd"
   },
   "outputs": [],
   "source": [
    "original =  [\"Good\" if x==1 else \"Bad\" for x in y_test[:20]]\n",
    "predicted = clf_log.predict(X_test_uni[:20])\n",
    "pred = []\n",
    "\n",
    "for i in predicted:\n",
    "  if i == 1:\n",
    "    k = \"Good\"\n",
    "    pred.append(k)\n",
    "  else:\n",
    "    k = \"Bad\"\n",
    "    pred.append(k)\n",
    "\n",
    "# dictionary of lists  \n",
    "dict = {'original_classlabel': original, 'predicted_classlabel': pred} \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbhwPb_setk3"
   },
   "outputs": [],
   "source": [
    "\n",
    "new = ['Logistic Regression with univariate feature selection ','LogisticRegression',0.7240,0.7213,0.5904,0.5928]\n",
    "results.loc[1] = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QbQE-j7dlsW"
   },
   "source": [
    "## Decision Tree with Removing features with low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPRReTCcdlT6",
    "outputId": "f236cf19-7924-4b83-b756-4ccc368040b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dept = [1, 5, 10, 50, 100, 500, 1000]\n",
    "min_samples =  [5, 10, 100, 500]\n",
    "\n",
    "\n",
    "param_grid={'min_samples_split':min_samples , 'max_depth':dept}\n",
    "dt = DecisionTreeClassifier()\n",
    "model = GridSearchCV(dt,param_grid,scoring='roc_auc',n_jobs=-1,cv=3)\n",
    "model.fit(X_train_var,y_train)\n",
    "print(\"optimal min_samples_split\",model.best_estimator_.min_samples_split)\n",
    "print(\"optimal max_depth\",model.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjtFAT4fesQ0",
    "outputId": "c16e8cb9-55fe-4d31-f5c0-06fb7ede6d99"
   },
   "outputs": [],
   "source": [
    "dt_model =model.best_estimator_\n",
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-t8EUaofgBU"
   },
   "outputs": [],
   "source": [
    "# #Trainig with the best value of C\n",
    "# dt_model.fit(X_train_var, y_train)\n",
    "# filename = r'C:\\Users\\ST-0008\\Desktop\\ITML12_Malicious_detection(NA)\\FINAL CODE\\FRONT END\\Malicious\\malicious_dt_var.pkl'\n",
    "# pickle.dump(dt_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUUj0XJ2gOid",
    "outputId": "90b20402-dccf-45c0-d87c-49a18fd0fedd"
   },
   "outputs": [],
   "source": [
    "#Printing the log-loss for both trian and test data\n",
    "train_loss = log_loss(y_train, dt_model.predict_proba(X_train_var)[:,1])\n",
    "test_loss  =log_loss(y_test, dt_model.predict_proba(X_test_var)[:,1])\n",
    "\n",
    "\n",
    "print(\"Log_loss on train data is :{}\".format(round(train_loss),4))\n",
    "print(\"Log_loss on test data is :{}\".format(round(test_loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "5UjCLmdCg1db",
    "outputId": "05f71ad6-c8cf-4c5a-ce3b-2a3afc295830"
   },
   "outputs": [],
   "source": [
    "#Testing AUC on Test data\n",
    "pred_test = dt_model.predict_proba(X_test_var)[:,1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_test)\n",
    "pred_train = dt_model.predict_proba(X_train_var)[:,1]\n",
    "fpr2,tpr2,thresholds2 = roc_curve(y_train,pred_train)\n",
    "\n",
    "#plot ROC curve\n",
    "x = plt.subplot( )\n",
    "x.plot(fpr1, tpr1, label ='Test ROC ,auc='+str(roc_auc_score(y_test,pred_test)))\n",
    "x.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred_train)))\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "x.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC on Test data is \" +str(roc_auc_score(y_test,pred_test)))\n",
    "print(\"AUC on Train data is \" +str(roc_auc_score(y_train,pred_train)))\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = le.classes_\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(y_test, pred_test.round()), index=class_names, columns=class_names )\n",
    "fig = plt.figure( )\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "B2Ns56Mfmpmj",
    "outputId": "186b1e87-666b-4820-d98e-94a8a04e7d76"
   },
   "outputs": [],
   "source": [
    "original =  [\"Good\" if x==1 else \"Bad\" for x in y_test[:20]]\n",
    "predicted = dt_model.predict(X_test_var[:20])\n",
    "pred = []\n",
    "\n",
    "for i in predicted:\n",
    "  if i == 1:\n",
    "    k = \"Good\"\n",
    "    pred.append(k)\n",
    "  else:\n",
    "    k = \"Bad\"\n",
    "    pred.append(k)\n",
    "\n",
    "# dictionary of lists  \n",
    "dict = {'original_classlabel': original, 'predicted_classlabel': pred} \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ71-arGqCEr"
   },
   "outputs": [],
   "source": [
    "new = ['Decision Tree with Removing features with low variance features','DecisionTreeClassifier',0.9399,0.9226,0.0,0.0]\n",
    "results.loc[2] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Pi0X-fFfYNL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0GlLT6AfYlx"
   },
   "source": [
    "## Decision Tree with Univariate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6kZHZUyfYl1",
    "outputId": "4ce8647b-9b8f-4305-80f2-eed22f916065"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dept = [1, 5, 10, 50, 100, 500, 1000]\n",
    "min_samples =  [5, 10, 100, 500]\n",
    "\n",
    "\n",
    "param_grid={'min_samples_split':min_samples , 'max_depth':dept}\n",
    "dt = DecisionTreeClassifier()\n",
    "model = GridSearchCV(dt,param_grid,scoring='roc_auc',n_jobs=-1,cv=3)\n",
    "model.fit(X_train_uni,y_train)\n",
    "print(\"optimal min_samples_split\",model.best_estimator_.min_samples_split)\n",
    "print(\"optimal max_depth\",model.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbxWMNxqfYmE",
    "outputId": "cedcf22e-14c0-4131-9bcf-829668d8cf6a"
   },
   "outputs": [],
   "source": [
    "dt_model =model.best_estimator_\n",
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_q9dM33tfYmP"
   },
   "outputs": [],
   "source": [
    "# #Trainig with the best value of C\n",
    "# dt_model.fit(X_train_uni, y_train)\n",
    "# filename = r'C:\\Users\\ST-0008\\Desktop\\ITML12_Malicious_detection(NA)\\FINAL CODE\\FRONT END\\Malicious\\malicious_dt_uni.pkl'\n",
    "# pickle.dump(dt_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xV4sEHbfhYY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcMU4GiZfYmf",
    "outputId": "ad4e48e3-8294-493f-ce89-940538321667"
   },
   "outputs": [],
   "source": [
    "#Printing the log-loss for both trian and test data\n",
    "train_loss = log_loss(y_train, dt_model.predict_proba(X_train_uni)[:,1])\n",
    "test_loss  =log_loss(y_test, dt_model.predict_proba(X_test_uni)[:,1])\n",
    "\n",
    "\n",
    "print(\"Log_loss on train data is :{}\".format(round(train_loss),4))\n",
    "print(\"Log_loss on test data is :{}\".format(round(test_loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "WM2e0hEdfYmv",
    "outputId": "ab55e6d3-77e9-4482-d9b8-3014cc4d8a1c"
   },
   "outputs": [],
   "source": [
    "#Testing AUC on Test data\n",
    "pred_test = dt_model.predict_proba(X_test_uni)[:,1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_test)\n",
    "pred_train = dt_model.predict_proba(X_train_uni)[:,1]\n",
    "fpr2,tpr2,thresholds2 = roc_curve(y_train,pred_train)\n",
    "\n",
    "#plot ROC curve\n",
    "x = plt.subplot( )\n",
    "x.plot(fpr1, tpr1, label ='Test ROC ,auc='+str(roc_auc_score(y_test,pred_test)))\n",
    "x.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred_train)))\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "x.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC on Test data is \" +str(roc_auc_score(y_test,pred_test)))\n",
    "print(\"AUC on Train data is \" +str(roc_auc_score(y_train,pred_train)))\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = le.classes_\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(y_test, pred_test.round()), index=class_names, columns=class_names )\n",
    "fig = plt.figure( )\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "Wg_cK3RxfYm1",
    "outputId": "7d640b55-9e3d-4531-a66d-b31d4b503059"
   },
   "outputs": [],
   "source": [
    "original =  [\"Good\" if x==1 else \"Bad\" for x in y_test[:20]]\n",
    "predicted = dt_model.predict(X_test_uni[:20])\n",
    "pred = []\n",
    "\n",
    "for i in predicted:\n",
    "  if i == 1:\n",
    "    k = \"Good\"\n",
    "    pred.append(k)\n",
    "  else:\n",
    "    k = \"Bad\"\n",
    "    pred.append(k)\n",
    "\n",
    "# dictionary of lists  \n",
    "dict = {'original_classlabel': original, 'predicted_classlabel': pred} \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwvrXYs_fYm7"
   },
   "outputs": [],
   "source": [
    "new = ['Decision Tree with univariate feature selection','DecisionTreeClassifier',0.885,0.8722,1.0,0.0]\n",
    "results.loc[3] = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtjwvYjdg-AQ"
   },
   "source": [
    "## Random Forest with Removing features with low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XMQSiE71-hY",
    "outputId": "98d52036-675f-49a6-f3e8-3ca4ccbe7830"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dept = [1, 5, 10, 50, 100, 500, 1000]\n",
    "n_estimators =  [20, 40, 60, 80, 100, 120]\n",
    "\n",
    "param_grid={'n_estimators':n_estimators , 'max_depth':dept}\n",
    "rf = RandomForestClassifier()\n",
    "rf = GridSearchCV(rf,param_grid,scoring='accuracy',n_jobs=-1,cv=3)\n",
    "rf.fit(X_train_var,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKEN2mPdjVsX",
    "outputId": "6beea087-72a9-4ae2-b1e5-e723e8dcc425"
   },
   "outputs": [],
   "source": [
    "print(\"optimal n_estimators\",rf.best_estimator_.n_estimators)\n",
    "print(\"optimal max_depth\",rf.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04m-5bUyhiL4",
    "outputId": "9eb1fea8-eaba-40b8-8673-48119301edc9"
   },
   "outputs": [],
   "source": [
    "rf_model =rf.best_estimator_\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbI-8_r1hiMA"
   },
   "outputs": [],
   "source": [
    "# #Trainig with the best value of C\n",
    "# rf_model.fit(X_train_var, y_train)\n",
    "# filename = r'C:\\Users\\ST-0008\\Desktop\\ITML12_Malicious_detection(NA)\\FINAL CODE\\FRONT END\\Malicious\\Malicious_rf.pkl'\n",
    "# pickle.dump(rf_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YkFMCO8hiMG",
    "outputId": "af07be39-7d39-433f-a488-d70960c5fd55"
   },
   "outputs": [],
   "source": [
    "#Printing the log-loss for both trian and test data\n",
    "train_loss = log_loss(y_train, rf_model.predict_proba(X_train_var)[:,1])\n",
    "test_loss  =log_loss(y_test, rf_model.predict_proba(X_test_var)[:,1])\n",
    "\n",
    "\n",
    "print(\"Log_loss on train data is :{}\".format(round(train_loss),4))\n",
    "print(\"Log_loss on test data is :{}\".format(round(test_loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "600F_D-ahiMN",
    "outputId": "1d55a44c-aae1-4a0d-956e-b62de3808a5f"
   },
   "outputs": [],
   "source": [
    "#Testing AUC on Test data\n",
    "pred_test = rf_model.predict_proba(X_test_var)[:,1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_test)\n",
    "pred_train = rf_model.predict_proba(X_train_var)[:,1]\n",
    "fpr2,tpr2,thresholds2 = roc_curve(y_train,pred_train)\n",
    "\n",
    "#plot ROC curve\n",
    "x = plt.subplot( )\n",
    "x.plot(fpr1, tpr1, label ='Test ROC ,auc='+str(roc_auc_score(y_test,pred_test)))\n",
    "x.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred_train)))\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "x.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC on Test data is \" +str(roc_auc_score(y_test,pred_test)))\n",
    "print(\"AUC on Train data is \" +str(roc_auc_score(y_train,pred_train)))\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = le.classes_\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(y_test, pred_test.round()), index=class_names, columns=class_names )\n",
    "fig = plt.figure( )\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "DkTE5kg8m32n",
    "outputId": "3c689fc8-1cef-4095-92a0-2a44d4300bb1"
   },
   "outputs": [],
   "source": [
    "original =  [\"Good\" if x==1 else \"Bad\" for x in y_test[:20]]\n",
    "predicted = rf_model.predict(X_test_var[:20])\n",
    "pred = []\n",
    "\n",
    "for i in predicted:\n",
    "  if i == 1:\n",
    "    k = \"Good\"\n",
    "    pred.append(k)\n",
    "  else:\n",
    "    k = \"Bad\"\n",
    "    pred.append(k)\n",
    "\n",
    "# dictionary of lists  \n",
    "dict = {'original_classlabel': original, 'predicted_classlabel': pred} \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQZ7F-HEjqwz"
   },
   "outputs": [],
   "source": [
    "new = ['Random Forest with Removing features with low variance features','RandomForestClassifier',0.9880,0.9569,0.0,0.0]\n",
    "results.loc[4] = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x79B9Qjqf4zD"
   },
   "source": [
    "## Random Forest with Univariate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqF1T2Ejf4zG",
    "outputId": "cd2c9548-25f1-4600-f11e-32736641603c"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dept = [1, 5, 10, 50, 100, 500, 1000]\n",
    "n_estimators =  [20, 40, 60, 80, 100, 120]\n",
    "\n",
    "param_grid={'n_estimators':n_estimators , 'max_depth':dept}\n",
    "rf = RandomForestClassifier()\n",
    "rf = GridSearchCV(rf,param_grid,scoring='accuracy',n_jobs=-1,cv=3)\n",
    "rf.fit(X_train_uni,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nm_Cgbdf4zP",
    "outputId": "fa357709-9aab-4fe5-8bd3-eb90524a29ed"
   },
   "outputs": [],
   "source": [
    "print(\"optimal n_estimators\",rf.best_estimator_.n_estimators)\n",
    "print(\"optimal max_depth\",rf.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZWqUA8Nf4zW",
    "outputId": "ace4d650-8fff-4a81-e37d-e6a4979b7ac3"
   },
   "outputs": [],
   "source": [
    "rf_model =rf.best_estimator_\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLKpe9OIf4za"
   },
   "outputs": [],
   "source": [
    "# #Trainig with the best value of C\n",
    "# rf_model.fit(X_train_uni, y_train)\n",
    "# filename = r'C:\\Users\\ST-0008\\Desktop\\ITML12_Malicious_detection(NA)\\FINAL CODE\\FRONT END\\Malicious/Malicious_rf_uni.pkl'\n",
    "# pickle.dump(rf_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaZlBB4vf4zk",
    "outputId": "26585444-316a-4503-e705-357c97a26411"
   },
   "outputs": [],
   "source": [
    "#Printing the log-loss for both trian and test data\n",
    "train_loss = log_loss(y_train, rf_model.predict_proba(X_train_uni)[:,1])\n",
    "test_loss  =log_loss(y_test, rf_model.predict_proba(X_test_uni)[:,1])\n",
    "\n",
    "\n",
    "print(\"Log_loss on train data is :{}\".format(round(train_loss),4))\n",
    "print(\"Log_loss on test data is :{}\".format(round(test_loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "F8IgQz3hf4zo",
    "outputId": "e339abb1-21b6-4641-c797-35a89f464a0c"
   },
   "outputs": [],
   "source": [
    "#Testing AUC on Test data\n",
    "pred_test = rf_model.predict_proba(X_test_uni)[:,1]\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_test)\n",
    "pred_train = rf_model.predict_proba(X_train_uni)[:,1]\n",
    "fpr2,tpr2,thresholds2 = roc_curve(y_train,pred_train)\n",
    "\n",
    "#plot ROC curve\n",
    "x = plt.subplot( )\n",
    "x.plot(fpr1, tpr1, label ='Test ROC ,auc='+str(roc_auc_score(y_test,pred_test)))\n",
    "x.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred_train)))\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "x.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC on Test data is \" +str(roc_auc_score(y_test,pred_test)))\n",
    "print(\"AUC on Train data is \" +str(roc_auc_score(y_train,pred_train)))\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Code for drawing seaborn heatmaps\n",
    "class_names = le.classes_\n",
    "df_heatmap = pd.DataFrame(confusion_matrix(y_test, pred_test.round()), index=class_names, columns=class_names )\n",
    "fig = plt.figure( )\n",
    "heatmap = sns.heatmap(df_heatmap, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "C4knvhvjf4zr",
    "outputId": "854c6a19-b294-4e5b-90e3-977408938e08"
   },
   "outputs": [],
   "source": [
    "original =  [\"Good\" if x==1 else \"Bad\" for x in y_test[:20]]\n",
    "predicted = rf_model.predict(X_test_uni[:20])\n",
    "pred = []\n",
    "\n",
    "for i in predicted:\n",
    "  if i == 1:\n",
    "    k = \"Good\"\n",
    "    pred.append(k)\n",
    "  else:\n",
    "    k = \"Bad\"\n",
    "    pred.append(k)\n",
    "\n",
    "# dictionary of lists  \n",
    "dict = {'original_classlabel': original, 'predicted_classlabel': pred} \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2mY52nWf4zw"
   },
   "outputs": [],
   "source": [
    "new = ['Random Forest with Univariate feature selection','RandomForestClassifier',0.87687,0.8889,0.0,0.0]\n",
    "results.loc[5] = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM7WjXZ2Al9b"
   },
   "source": [
    "## Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "JKKickmMAj7r",
    "outputId": "0907fc8b-9d22-4101-db75-ccee6f340291"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Malicious.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
